---
title: "BDA - Project"
author: "Leo Vuylsteker (876373), Anar Abetayeva (903068)"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1

---
```{r, results='hide', echo=TRUE, message = FALSE}
library(aaltobda)
library(rstan)
library(rstantools)
library(ggplot2)
library(loo)
library(bayesplot)
library(glue)

df_res <- read.csv('ligue1-20192020-results.csv')
df_next <- read.csv('ligue1-20192020-cancelled-games.csv')
df_teams <- read.csv('ligue1-20192020-teams.csv')

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r}
model_separate <- readRDS("separate_model_opt.rds")
model_hierarchical <- readRDS("model_hierarchical_5_15.rds") 
```

# 1. Introduction describing

In 2020, because of the COVID-19 outbreak, the football world has experienced one of the biggest economic crisis in its history and national championships all around Europe had to be interrupted for several weeks. In an economy that revolves mostly around Sports broadcasting contracts, match day revenues, and player sales, stop playing games for a long period of time is an unbearable situation. Most leagues in Europe started playing games without home and away fans, before and after their domestic lockdowns, despite the risks of contamination between the players. However, in some countries, such as France, Scotland and Belgium, national leagues had to be 'freezed' (=terminated) approximately 10 match days short compared to a regular season, not without controversy. Indeed, on top of the aforementioned economics impacts, a lot of football club representatives in those countries considered themselves wronged by this decision. Based on the following fixtures and momentum in the past few games, they felt like their team could have reached much more desirable spots in the league table (that would have allowed them to avoid relegation or to reach promotion and european competitions for instance) if they were allowed to play the remaining games. And because some teams would have arguably had easier fixtures coming up than others it is very difficult to deny those claims. In addition, there are usually a lot of surprises towards the end of a season because of the uncertain nature of the sport. In the French Ligue 1 for instance, 71% of the teams have changed places over the last 10 games of season historically.

Also in France, numerous alternatives were proposed by club owners to avoid 'freezing' the season, and even though they were not retained in the end, one did stand out from a scientific point of view... In an interview with the French media L'Equipe in april 2020, Lille president Gerard Lopez said 'We can imagine a more scientific calculation model in which we would simulate the results of the last ten games, according to the points taken at home or away this season, to obtain a ranking after 38 games'. Such models based on Bayesian theory not only exist, but are used by bookmakers to predict the outcomes of a season. But how reliable are they in reality? What kind of impact thoses predictions would have had on the final Ligue 1 table? Would Lopez's team have managed to secure a place in the Champions League at the expense of Rennes or Marseille in the remaining matches? We will try to answer those questions in this report by creating our own 2020 Ligue 1 simulation model.



But first, let's start introducing a little bit of abstraction to understand how we can predict the outcome of a football game. Unfortunately for us, there would never be a perfectly accurate answer to that question as football is such an unpredictable game. The sport itself is full of David vs Goliath stories and there would be so much parameters to took into account that it would not be a dead end to try reaching very high level accuracies for this project.
The approach we had was much simpler. Intuitively, we hypothesized that each team in France had some abilities to attack and defend that we would like to evaluate based on the previous results. Then, when looking at a fixture we could combine those values to know how much goals the home and away sides are likely to score or concede. We also added a some fixed bonus to the home side to represente the home advantage in football. Indeed, as they are playing in front of their own fans, it has been shown that home teams tend to win a little bit more frequently and we wanted to take this information into account as well. Knowing this information is particularly important in our case study because it enables us to simulate games were the home advantage's influence is removed (games that are played without public because of the COVID restrictions for instance).



# 2. Description of the data and the analysis problem. 


For this analysis, we have used scoresheets data allongside the composition of the remaining fixtures of the 2019-2020 Ligue 1 season from the internet website Soccerway.com.
Considering the fact that our research question is derived from a recent event at the time of writting, it is unlikely to our knowledge than other existing analysis have been made on the same dataset. However, it is worth to mention that our work takes inspiration from other academic works that have been performed on different leagues and seasons, works that are themselves based on decades of scientific research made on sports analytics and prediction models.



# 3. Description of the models


## Hierarchical model

As we have said, our work took inspiration from other academic publications. In particular, "Bayesian hierarchical model for the prediction of football results" by Baio and Blangiardo has been an great source of knowledge and a corner stone for the models we have decided to implement. As its name suggest it, the paper uses a hierarchical model to predict the attacking and defensive strenght of the teams.

The hyperpriors $\mu_{att}$,$\tau_{att}$ and $\mu_{def}$,$\tau_{def}$  define the mean and standard deviation of a team's capabibilities on both the attacking and defensive side.
Leading the following description of the team-specific effects (for each team $t$ in $1,...,T$) modelled as exchangeable from a common normal distribution:
$$
\text{att}_{t} \sim \text{Normal}(\mu_{att}, \tau_{att}) \\
\text{def}_{t} \sim \text{Normal}(\mu_{def}, \tau_{def}) \\
$$
The home advantage is modelled by the random variable home.

Thus, the scoring rate $\theta$ for both team during the g-th game of the season is defined with a log-linear random effect model:
$$
\text{log}(\theta_{g,1}) = \text{home} + \text{att}_{h(g)} + \text{def}_{a(g)} \\
\text{log}(\theta_{g,2}) = \text{att}_{a(g)} + \text{def}_{h(g)}
$$
How should one understand this formula ? Well, notice in the second formula how the value of the scoring intensity of the away team $\theta_{g,2}$ is high if its attacking capability $\text{att}_{a(g)}$ is high or if the defensive capability of the home team $\text{def}_{h(g)}$ is high (defensive capabilities are always negative in this definition). Same thing could be said for the first formula but with the addition of the team advantage variable.

There is a small catch however. In order to achieve identifiability of the above model parameters and facilitate the interpretation the results in the end, we have to use a standard set of constraints on the $\text{att}$ and $\text{def}$ variables. The authors opted for a sum-to-zero constraint (which is simply equivalent to centering the variables $\text{att}$ and $\text{def}$):
$$
\sum_{t=1}^T \text{att}_{t} = 0 \\
\sum_{t=1}^T \text{def}_{t} = 0
$$

Finally, since the goals occurring in the game are rare events with now known rates $\theta_{g,1}$ and $\theta_{g,2}$, Poisson distributions are particularly suited to transform those rates in an actual number of goals for each side $y_{g,1}$ and $y_{g,2}$:
$$
y_{gj} | \theta_{gj} \sim \text{Poisson}(\theta_{gj}),
$$

In the end, the model can be sumarized by the following sketch that is featured in the paper:



In our own model, we finded out that the attacking and defending hyperpriors did not need to be defined as strictly as they are in the paper. In particular, since we only care about the spreading of the team-specific effect $att_t$ and $def_t$ to differenciate the teams (the information about the mean values are lost with the sum-to-zero constraint), we figured out that we might as well eliminate the hyperpriors relative to the mean values and model the team specific effect simply as:
$$
\text{att}_t \sim \text{Normal}(0, \tau_{att}) \\
\text{def}_t \sim \text{Normal}(0, \tau_{def})
$$
The MCMC sampling has proved to be way faster with our model than the one in the paper with the addition of $\mu_{att}$ and $\mu_{def}$ while maintaining sensible results.

## Separate model

In football, the assumption that the team capabilities are sampled from common hyperprior distribution is difficult to justify in the context of the French Ligue 1. Indeed, the levels of the teams in the top flight are far from being homogeneous. There is Paris Saint-Germain, alone at the very top of the charts, a bunch of good teams competing for the european spots like Marseille, Rennes, Lille, Nice, Monaco or Lyon and other teams like Toulouse which are simply not good enough. The issue arising from hierarchical models in this case is that the best teams appear to perform worse and the worst teams appear to perform better in the simulations since in both case the results are pulled towards the mean values for the team cappabilities. To mitigate this problem, the authors have implemented 3 different bins with dinstinct hyperpriors values corresponding to the top, the mid-table, and the bottom teams. However, the number of bins is purely arbitrary and based on a priori definition spread by the sports media. In Ligue 1, it would be difficult to know the optimal number of bins and that is why we opted for a completely separate (un-pooled) model which consider each team as unique and should produce better predictions than the hierarchical model, at the cost of a harder MCMC sampling.

Apart from this and the supression of the hyperpriors, the rest of the model is similar.



# 4. Informative or weakly informative priors, and justification of their choices.


## Hierarchical model

Since it is really difficult to assess objectively the home advantage and the attacking and offensive capabilities of the teams as we do not have a population basis, we decided to stick to the following noninformative set of hyperpriors and priors:

$$
\mu_{att} \sim \text{Normal}(0, 100) \\
\tau_{att} \sim \text{Gamma}(0.1, 0.1) \\
\mu_{def} \sim \text{Normal}(0, 100) \\
\tau_{def} \sim \text{Gamma}(0.1, 0.1)
$$

$$
\text{home} \sim \text{Normal}(0,100)
$$
This way, we hoped that the prior distributions will play a minimal role in the posterior distributions and that the inferences will be unaffected by information external to the actual data.

## Separate model

For the same reasons, the team-specific effects (for each team $t$ in $1,...,T$) are modelled as follows in the separate model:
$$
\text{att}_t \sim \text{Normal}(0, 1) \\
\text{def}_t \sim \text{Normal}(0, 1)
$$



# 5. Stan code.

## Common model design


The number of goals scored by the home and by the away team in the g-th game of the season, where g indicates the game and $g=1,..,G=279$ is modeled as independent Poisson distribution for each team. We model the vector of observed goals $y=(y_{g1}, y_{g2})$ as


$y_{gj} | \lambda_{gj} \sim Poisson(\lambda_{gj})$


where the parameters $\lambda=(\lambda_{g1}, \lambda_{g2})$ represent the scoring intensity in the g-th game for the team playing at home (j=1) and away (j=2), respectively. These parameters are modeled using a log-linear random effect model


$log(\lambda_{g1}) = home + att_{h(g)} + def_{a(g)}$, $\lambda_{g1} = \exp(home + att_{h(g)} + def_{a(g)})$


$log(\lambda_{g2}) = att_{a(g)} + def_{h(g)}$, $\lambda_{g2} = \exp(att_{a(g)} + def_{h(g)})$



where $h(g)$ and $a(g)$ indicates the teams which played g-th game, and $h(g), a(g) = 1,..,T$ 




## Separate model

![The DAG representation of the Separate model](/home/anar/BDA/R_projects/project/img/sep_model.png) 


$home \sim N(0,100)$


For each $t = 1,..,T$, the team specific effects are modeled from separate distributions:


$att_t \sim N(0, 1)$, $def_t \sim N(0, 1)$


We introduce sum-to zero constraint on the team-specific parameters:


$\sum_{t=1}^{T} att_t = 0$, $\sum_{t=1}^{T} def_t = 0$

The rest of the procedure is same for both models and described in common model design part.

```{stan, output.var="separate_model", eval = FALSE}
data {
  int<lower=0> N;
  int<lower=0> nteams;
  int y1[N];
  int y2[N];
  int hometeam[N];
  int awayteam[N];
  
  int<lower=0> next_N;
  int next_hometeam[next_N];
  int next_awayteam[next_N];
}


parameters {
  //parameters
  real<lower=0> home;
  vector<lower=0>[nteams] att_star;
  vector<lower=0>[nteams] def_star;
}


transformed parameters{
  vector[nteams] att;
  vector[nteams] def;
  real<lower=0> lambda[N, 2];
  
  // center attacking and defending (sum to zero)
  for (t in 1:nteams){
    att[t] = att_star[t] - mean(att_star);
    def[t] = def_star[t] - mean(def_star);
  }
  
  for (g in 1:N){
    // Average Scoring intensities (accounting for mixing components)
    lambda[g,1] = exp(home + att[hometeam[g]] + def[awayteam[g]]);
    lambda[g,2] = exp(att[awayteam[g]] + def[hometeam[g]]);
  }
}


model {
  home ~ normal(0,100);
  
  for (t in 1:nteams){
    att_star[t] ~ normal(0, 1);
    def_star[t] ~ normal(0, 1);
  }
  
  for (g in 1:N) {
    // Observed number of goals scored by each team
    y1[g] ~ poisson(lambda[g,1]);
    y2[g] ~ poisson(lambda[g,2]);
  }
}


generated quantities {
  real<lower=0> next_lambda[next_N, 2];
  int next_y1[next_N];
  int next_y2[next_N];
  vector[N] log_lik[2];
  
  for (g in 1:next_N){
    // Average Scoring intensities (accounting for mixing components)
    next_lambda[g,1] = exp(att[next_hometeam[g]] + def[next_awayteam[g]]);
    next_lambda[g,2] = exp(att[next_awayteam[g]] + def[next_hometeam[g]]);
    
    // Predicted number of goals scored by each team
    next_y1[g] = poisson_rng(next_lambda[g,1]);
    next_y2[g] = poisson_rng(next_lambda[g,2]);
  }
  
   for (g in 1:N){
      log_lik[1,g] = poisson_lpmf(y1[g] | lambda[g,1]);
      log_lik[2,g] = poisson_lpmf(y2[g] | lambda[g,2]);
}
  
  
}

```

## Hierarchical model


The key difference between separate and hierarchical model is that in hierarchical model team-specific effects are modeled from common distribution. This model design infers two outcome variables are correlated, as teams playing one game interact with each other.

![The DAG representation of the Hierarchical model](/home/anar/BDA/R_projects/project/img/hier_model.png) 

$home \sim N(0,100)$


For each $t = 1,..,T$, the team specific effects are modeled as exchangeable from a common distribution:


$att_t \sim N(0, \sigma_{att})$, $def_t \sim N(0, \sigma_{def})$


We introduce sum-to zero constraint on the team-specific parameters:


$\sum_{t=1}^{T} att_t = 0$, $\sum_{t=1}^{T} def_t = 0$

The hyper-priors of the attack and defense effects are modeled independently using flat prior distributions:

$\sigma_{att} \sim Gamma(0.1,0.1)$, $\sigma_{def} \sim Gamma(0.1,0.1)$

The rest of the model design is common for separate and hierarchical models.


```{stan, output.var="hierarchical_model", eval = FALSE}
data {
  int<lower=0> N;
  int<lower=0> nteams;
  int y1[N];
  int y2[N];
  int hometeam[N];
  int awayteam[N];
  
  int<lower=0> next_N;
  int next_hometeam[next_N];
  int next_awayteam[next_N];
}


parameters {
  //hyperparameters
  // real mu_att;
  real<lower=0> sigma_att;
  
  // real mu_def;
  real<lower=0> sigma_def;
  
  //parameters
  real<lower=0> home;
  
  vector<lower=0>[nteams] att_star;
  vector<lower=0>[nteams] def_star;
}


transformed parameters{
  vector[nteams] att;
  vector[nteams] def;
  real<lower=0> lambda[N, 2];
  
  // center attacking and defending (sum to zero)
  for (t in 1:nteams){
    att[t] = att_star[t] - mean(att_star);
    def[t] = def_star[t] - mean(def_star);
  }
  
  for (g in 1:N){
    // Average Scoring intensities (accounting for mixing components)
    lambda[g,1] = exp(home + att[hometeam[g]] + def[awayteam[g]]);
    lambda[g,2] = exp(att[awayteam[g]] + def[hometeam[g]]);
  }
}


model {
  home ~ normal(0,100);
  
  // mu_att ~ normal(0,100) T[0,];
  sigma_att ~ gamma(5,15);
  
  // mu_def ~ normal(0,100) T[0,];
  sigma_def ~ gamma(5,15);
  
  for (t in 1:nteams){
    att_star[t] ~ normal(0, sigma_att); //normal(mu_att, sigma_att) T[0,];
    def_star[t] ~ normal(0, sigma_def); //normal(mu_def, sigma_def) T[0,];
  }
  
  for (g in 1:N) {
    // Observed number of goals scored by each team
    y1[g] ~ poisson(lambda[g,1]);
    y2[g] ~ poisson(lambda[g,2]);
  }
}


generated quantities {
  real<lower=0> next_lambda[next_N, 2];
  int next_y1[next_N];
  int next_y2[next_N];
  vector[N] log_lik[2];
  
  for (g in 1:next_N){
    // Average Scoring intensities (accounting for mixing components)
    next_lambda[g,1] = exp(att[next_hometeam[g]] + def[next_awayteam[g]]);
    next_lambda[g,2] = exp(att[next_awayteam[g]] + def[next_hometeam[g]]);
    
    // Predicted number of goals scored by each team
    next_y1[g] = poisson_rng(next_lambda[g,1]);
    next_y2[g] = poisson_rng(next_lambda[g,2]);
  }
  
     for (g in 1:N){
      log_lik[1,g] = poisson_lpmf(y1[g] | lambda[g,1]);
      log_lik[2,g] = poisson_lpmf(y2[g] | lambda[g,2]);
}
}
```


Here, using unobservable hyperparameters $\sigma_{att}$, $\sigma_{def}$, the inherent hierarchical nature implies a form of correlation between observable variables $y_{g1}$ and $y_{g2}$. 

# 6. How to the Stan model was run, that is, what options were used. This is also more clear as combination of textual explanation and the actual code line.

```{r eval=FALSE}
stan_data <- list(N=nrow(df_res), nteams=length(unique(df_res$home.team)), y1=df_res$h.g., y2=df_res$a.g, hometeam=df_res$home.team, awayteam=df_res$away.team,
                  next_N=nrow(df_next),
                  next_hometeam=df_next$home.team, next_awayteam=df_next$away.team, sigma_att=1, sigma_def=1)
sm <- rstan::stan_model(file = "football_separate.stan")
model_separate <- rstan::sampling(sm, data = stan_data, chains=4, iter=12000, seed=42)
```

```{r eval=FALSE}
sm <- rstan::stan_model(file = "model_init.stan")
model_hierarchical <- rstan::sampling(sm, data = stan_data, iter=10000, warmup=2000,
                                      control = list(adapt_delta = 0.99, max_treedepth = 15), seed=42)
```
# 7. Convergence diagnostics (R, ESS, divergences) and what was done if the convergence was not good with the first try.

## $\hat{R}$: potential scale reduction statistic

$\hat{R}$ statistic measures the ratio of the average variance of draws within each chain to the variance of the pooled draws across chains. In case the chains have not converged to a common distribution, the $\hat{R}$ statistic will be greater than one. 

We can see from the plot of $\hat{R}$ values of separate model that all values are close to one, thus the model have converged. 
```{r}
rhats_sep <- rhat(model_separate)
mcmc_rhat(rhats_sep)
```
We also conclude that hierarchical model have converged as all of $\hat{R}$ values are around 1, thus the model have converged.
```{r}
rhats_hier <- rhat(model_hierarchical)
mcmc_rhat(rhats_hier)
```


## Effective sample size
The effective sample size is an estimate of the number of independent draws from the posterior distribution of the estimand of interest. The draws within a Markov chain are not independent if there is autocorrelation, the effective sample size, $n_{eff}$, is usually smaller than the total sample size, $N$. The larger the ratio on $n_{eff}$ to $N$ the better. If there are ratios $\frac{n_{eff}}{N}$ less than 0.1, there might be autocorrelation.

From the plot of ratios on $n_{eff}$ to $N$ of separate model we can see that all ratios are more than 0.1, we can conclude that there is no autocorrelation. 
```{r}
ratios_sep <- neff_ratio(model_separate)
mcmc_neff(ratios_sep)
```

For hierarchical model there are some parameters with ratio on $n_{eff}$ to $N$ less than 0.1. We probably have autocorrelation for those parameters, and it means that model is needed to be run with more iterations. 
```{r}
ratios_hier <- neff_ratio(model_hierarchical)
mcmc_neff(ratios_hier)
```
## Divergences
There were no divergent iterations for separate model.
```{r}
divergent <- get_sampler_params(model_separate, inc_warmup=FALSE)[[1]][,'divergent__']
sum(divergent)
```
Hierarchical model had no divergent iterations as well.
```{r}
divergent <- get_sampler_params(model_hierarchical, inc_warmup=FALSE)[[1]][,'divergent__']
sum(divergent)
``` 
For separate model during the first try the default number of iterations which is 2000 were performed, and Bulk Effective Sample Size was too low. We needed to increase the number of iterations to 12000 as with fewer iterations $\hat{R}$ values were acceptable, but ratios on $n_{eff}$ to $N$ for some parameters were still lower than 0.1 with less number of iterations.



# 8. Posterior predictive checks and what was done to improve the model.


## Separate model posterior predictive checks
Compare histogram of $y_{g1}$ and $y_{g2}$ to histograms of $y\_rep_{g1}$ and $y\_rep_{g2}$

```{r}
y_rep1_sep <- as.matrix(model_separate, pars='y_rep1')
y_1 <- df_res$h.g.
ppc_hist(y_1, y_rep1_sep[1:8, ], bandwidth=1)
```

```{r}
y_rep2_sep <- as.matrix(model_separate, pars='y_rep2')
y_2 <- df_res$a.g.
ppc_hist(y_2, y_rep2_sep[1:8, ], bandwidth=1)
```

Compare density estimate of $y_{g1}$ and $y_{g2}$ to density estimates of a bunch of $y\_rep_{g1}$ and $y\_rep_{g2}$

```{r}
ppc_dens_overlay(y_1, y_rep1_sep[1:100, ])
```

```{r}
ppc_dens_overlay(y_2, y_rep2_sep[1:100, ])
```

Compare proportions of zeros in $y_{g1}$ and $y_{g2}$ to the distribution of that proportion over all $y\_rep_{g1}$ and $y\_rep_{g2}$

```{r}
prop_zero <- function(x) mean(x == 0)
print(prop_zero(y_1))
ppc_stat(y_1, y_rep1_sep, stat="prop_zero")
```

```{r}
print(prop_zero(y_2))
ppc_stat(y_2, y_rep2_sep, stat="prop_zero")
```

Two statistics in a scatterplot

```{r}
ppc_stat_2d(y_1, y_rep1_sep, stat=c("mean", "sd"))
```

```{r}
ppc_stat_2d(y_2, y_rep2_sep, stat=c("mean", "sd"))
```

Distribution of predictive errors

```{r}
ppc_error_hist(y_1, y_rep1_sep[1:4, ], bandwidth=1) + xlim(-10, 10)
```
```{r}
ppc_error_hist(y_2, y_rep2_sep[1:4, ], bandwidth=1) + xlim(-10, 10)
```

## Hierarchical model posterior predictive checks
Compare histogram of $y_{g1}$ and $y_{g2}$ to histograms of $y\_rep_{g1}$ and $y\_rep_{g2}$

```{r}
y_rep1_hier <- as.matrix(model_hierarchical, pars='y_rep1')
ppc_hist(y_1, y_rep1_hier[1:8, ], bandwidth=1)
```

```{r}
y_rep2_hier <- as.matrix(model_hierarchical, pars='y_rep2')
ppc_hist(y_2, y_rep2_hier[1:8, ], bandwidth=1)
```

Compare density estimate of $y_{g1}$ and $y_{g2}$ to density estimates of a bunch of $y\_rep_{g1}$ and $y\_rep_{g2}$

```{r}
ppc_dens_overlay(y_1, y_rep1_hier[1:100, ])
```

```{r}
ppc_dens_overlay(y_2, y_rep2_hier[1:100, ])
```

Compare proportions of zeros in $y_{g1}$ and $y_{g2}$ to the distribution of that proportion over all $y\_rep_{g1}$ and $y\_rep_{g2}$

```{r}
prop_zero <- function(x) mean(x == 0)
print(prop_zero(y_1))
ppc_stat(y_1, y_rep1_hier, stat="prop_zero")
```

```{r}
print(prop_zero(y_2))
ppc_stat(y_2, y_rep2_hier, stat="prop_zero")
```

Two statistics in a scatterplot

```{r}
ppc_stat_2d(y_1, y_rep1_hier, stat=c("mean", "sd"))
```

```{r}
ppc_stat_2d(y_2, y_rep2_hier, stat=c("mean", "sd"))
```

Distribution of predictive errors

```{r}
ppc_error_hist(y_1, y_rep1_hier[1:4, ], bandwidth=1) + xlim(-10, 10)
```
```{r}
ppc_error_hist(y_2, y_rep2_hier[1:4, ], bandwidth=1) + xlim(-10, 10)
```



# 9. Model comparison.


Here we assess whether there are differences between the proposed models with regard to the $elpd_{loo-cv}$, and if so, which model should be selected according to PSIS-LOO.
Before we compare the difference in $elpd_{loo-cv}$, we have to make sure that all estimate are reliable. We can analyze these using the Pareto k diagnostic, which is intended to estimate how far an individual leave-one-out distribution is from the full distribution. The Pareto k estimate, in turn, is a diagnostic for Pareto smoothed importance sampling, which is used to approximate the leave-one pointwise predictive distribution for the leave one out cross validation. Data points with k-value greater than 0.7 are often the result of model misspecification and frequently correspond to data points that would be considered outliers. If $k>0.7$, it means leaving out an observation significantly changes the posterior and importance sampling fails to produce reliable estimate. If the $k<0.7$ we can consider the corresponding estimate with a relatively high accuracy. However, large k-values are a useful indicator of model misspecifications, small k-values are not a guarantee that model is well-specified. In case of proposed two models, the PSIS-LOO estimates both for separate and hierarchical models are reliable, and we can compare two models. When we compare models, we conclude that the difference in $elpd_{loo-cv}$ and its scale relative to the approximate standard error of the difference indicates a preference for the hierarchical model. 


Computing the PSIS-LOO elpd values and the k-values for separate model.
```{r}
compute_psis_loo <- function(model){
  log_lik <- extract_log_lik(model, merge_chains = FALSE)
  r_eff <- relative_eff(exp(log_lik), cores = 2)
  return(loo(log_lik, r_eff = r_eff, cores = 2))
}

loo_sep <- compute_psis_loo(model_separate)
```

Visualization of k-values for separate model.
```{r}
plot(loo_sep,
     diagnostic = c("k", "n_eff"),
     label_points = FALSE,
     main = "PSIS diagnostic plot separate model"
)
```
PSIS-LOO values for separate model.
```{r}
loo_sep
```



Computing the PSIS-LOO elpd values and the k-values for hierarchical model.
```{r}

loo_hierarchical <- compute_psis_loo(model_hierarchical)
```
Visualization of k-values for hierarchical model.
```{r}
plot(loo_hierarchical,
     diagnostic = c("k", "n_eff"),
     label_points = FALSE,
     main = "PSIS diagnostic plot hierarchical model")
```
PSIS-LOO values for hierarchical model.
```{r}
loo_hierarchical
```


Comparing the models on expected log predictive density
```{r}
loo_compare(loo_sep, loo_hierarchical)
```



# 10. Predictive performance assessment.


Based on the results that we obtained from the generated quantities blocks, a nice qualitative performance assessment that we can do is to look at the predicted ligue 1 table and see if the results make sense in both models.

First of all let's build and display the league table from the matches that were actually played during the 2019-2020 season:
```{r}
current_table <- data.frame(team = df_teams$team, MP = rep(0, 20), W = rep(0, 20), D = rep(0, 20),
                            L = rep(0, 20), F = rep(0, 20), A = rep(0, 20), GD = rep(0, 20), P = rep(0, 20))

# fill the table with the information of df_res
for(row in 1:nrow(df_res)){
  home_idx = df_res[row, 'home.team']
  away_idx = df_res[row, 'away.team']
  
  # +1 match played
  current_table[home_idx, 'MP'] = current_table[home_idx, 'MP'] + 1
  current_table[away_idx, 'MP'] = current_table[away_idx, 'MP'] + 1
  
  g_home = df_res[row, 'h.g.']
  g_away = df_res[row, 'a.g.']
  
  # update goals for and against
  current_table[home_idx, 'F'] = current_table[home_idx, 'F'] + g_home
  current_table[home_idx, 'A'] = current_table[home_idx, 'A'] + g_away
  current_table[away_idx, 'F'] = current_table[away_idx, 'F'] + g_away
  current_table[away_idx, 'A'] = current_table[away_idx, 'A'] + g_home
  
  diff = g_home - g_away
  
  # update goal difference
  current_table[home_idx, 'GD'] = current_table[home_idx, 'GD'] + diff
  current_table[away_idx, 'GD'] = current_table[away_idx, 'GD'] - diff
  
  if(diff > 0){
    # Win for the home side
    current_table[home_idx, 'W'] = current_table[home_idx, 'W'] + 1
    current_table[away_idx, 'L'] = current_table[away_idx, 'L'] + 1
    # A win worths 3 points
    current_table[home_idx, 'P'] = current_table[home_idx, 'P'] + 3
  }
  if(diff < 0){
    # Win for the away side
    current_table[home_idx, 'L'] = current_table[home_idx, 'L'] + 1
    current_table[away_idx, 'W'] = current_table[away_idx, 'W'] + 1
    # A win worths 3 points
    current_table[away_idx, 'P'] = current_table[away_idx, 'P'] + 3
  }
  if(diff == 0){
    # Draw
    current_table[home_idx, 'D'] = current_table[home_idx, 'D'] + 1
    current_table[away_idx, 'D'] = current_table[away_idx, 'D'] + 1
    # A draw worths 1 point for each side
    current_table[home_idx, 'P'] = current_table[home_idx, 'P'] + 1
    current_table[away_idx, 'P'] = current_table[away_idx, 'P'] + 1
  }
}

sorted_current_table <- current_table[order(current_table$P, current_table$GD, current_table$F, -current_table$A, decreasing = TRUE), ]
sorted_current_table <- data.frame(pos = 1:20, sorted_current_table)

sorted_current_table
```


Then we could add the information contained in our predictions:
```{r}
sep_predicted_table <- data.frame(current_table)
hie_predicted_table <- data.frame(current_table)

predict <- function(table, model, threshold=0.1){
    # fill the table with the information of df_next and the predictions of both models
    pred_table = summary(model)$summary
    
    for(row in 1:nrow(df_next)){
      home_idx = df_next[row, 'home.team']
      away_idx = df_next[row, 'away.team']
      
      # +1 match played
      table[home_idx, 'MP'] = table[home_idx, 'MP'] + 1
      table[away_idx, 'MP'] = table[away_idx, 'MP'] + 1
      
      g_home = pred_table[glue('next_y1[{row}]'),'mean']
      g_away = pred_table[glue('next_y2[{row}]'),'mean']
      
      # update goals for and against
      table[home_idx, 'F'] = table[home_idx, 'F'] + g_home
      table[home_idx, 'A'] = table[home_idx, 'A'] + g_away
      table[away_idx, 'F'] = table[away_idx, 'F'] + g_away
      table[away_idx, 'A'] = table[away_idx, 'A'] + g_home
      
      diff = g_home - g_away
      if(abs(diff) < threshold){
        diff = 0
      }
      
      # update goal difference
      table[home_idx, 'GD'] = table[home_idx, 'GD'] + diff
      table[away_idx, 'GD'] = table[away_idx, 'GD'] - diff
      
      if(diff > 0){
        # Win for the home side
        table[home_idx, 'W'] = table[home_idx, 'W'] + 1
        table[away_idx, 'L'] = table[away_idx, 'L'] + 1
        # A win worths 3 points
        table[home_idx, 'P'] = table[home_idx, 'P'] + 3
      }
      if(diff < 0){
        # Win for the away side
        table[home_idx, 'L'] = table[home_idx, 'L'] + 1
        table[away_idx, 'W'] = table[away_idx, 'W'] + 1
        # A win worths 3 points
        table[away_idx, 'P'] = table[away_idx, 'P'] + 3
      }
      if(diff == 0){
        # Draw
        table[home_idx, 'D'] = table[home_idx, 'D'] + 1
        table[away_idx, 'D'] = table[away_idx, 'D'] + 1
        # A draw worths 1 point for each side
        table[home_idx, 'P'] = table[home_idx, 'P'] + 1
        table[away_idx, 'P'] = table[away_idx, 'P'] + 1
      }
    }
    
    table <- table[order(table$P, table$GD, table$F, -table$A, decreasing = TRUE), ]
    table <- data.frame(pos = 1:20, table)
    keeps <- c('pos', 'team', 'MP', 'W', 'D', 'L', 'P')
    return(table[,keeps])
}

predict(table=hie_predicted_table, model = model_hierarchical, threshold=0.11)
predict(table=sep_predicted_table, model = model_separate, threshold=0.2)
```


The information relative to the goal scored and conceided have been hidden since the values are modeled with the mean values that do not correspond to integers. Because of this, the goal difference predicted for games is also never exactly equal to 0 (meaning that the model will never predict draws). Hence, we have manually tuned threshold values to ensure that the proportion of draws predicted is close enough to the proportion of draws observed in real life.

As we expected the separate model does not have the tendency of the hierarchical to pull the extreme points values towards the mean: 
* The hierarchical model predicts that a good team like Lille will obtain 20 additional points each while the separate models is more optimistic for then with 24 points.
* The hierarchical model predicts that Dijon, a team playing in the bottom half of the table, will obtain 6 additional points each while the separate models is more pessimistic with only 4 points.
* Other major changes in prediction included a tighter 3rd place race between Lille and Rennes with the separate model.

However, for the most part, both models predicted a very similar final tables where the major changes compared to the 'freezed season' include a surprising come back from Bordeaux which would have seen them ending up in the Europa league spots alongside Lyon (instead of Nice and Reims). Both also predict a major fall from grace for Reims and Angers, typically defensive sides, but also Monaco one of the best attack of the division but who struggled a lot on the defensive side.
Here are the predicted progressions and regressions in the table compared to the freezed table:
```{r}

difference <- function(actual_table, pred_table){
  diff_table <- data.frame(team=pred_table$team, true_position=rep(0,20), position_change = rep(0, 20), points_change = rep(0,20))
    for(actual_pos in 1:nrow(actual_table)){
      team = sorted_current_table[actual_pos, 'team']
      actual_points = sorted_current_table[actual_pos, 'P']
      pred_points = pred_table[pred_table$team == team, 'P']
      pred_pos = pred_table[pred_table$team == team, 'pos']
      diff_table[pred_pos, 'position_change'] = actual_pos - pred_pos
      diff_table[pred_pos, 'points_change'] = pred_points - actual_points
      diff_table[pred_pos, 'true_position'] = actual_pos
    }
  diff_table <- diff_table[order(diff_table$position_change, decreasing = TRUE), ]
  return(diff_table)
}

difference(sorted_current_table, predict(table=hie_predicted_table, model = model_hierarchical, threshold=0.11))
difference(sorted_current_table, predict(table=sep_predicted_table, model = model_separate, threshold=0.2))

```


## 11. Sensitivity analysis with respect to prior choices (i.e. checking whether the result changes a lot if prior is changed)

$att_t \sim N(0, 100)$, $def_t \sim N(0, 100)$

```{r}
model_separate_100 <- readRDS('model_separate_100.rds')

loo_sep_100 <- compute_psis_loo(model_separate_100)
```
$att_t \sim N(0, 10)$, $def_t \sim N(0, 10)$

```{r}
model_separate_10 <- readRDS('model_separate_10.rds')
loo_sep_10 <- compute_psis_loo(model_separate_10)

loo_compare(loo_sep_100, loo_sep_10, loo_sep)
```


$\sigma_{att} \sim Gamma(10,10)$, $\sigma_{def} \sim Gamma(10,10)$

```{r}
model_hierarchical_10_10 <- readRDS('model_hierarchical_10_10.rds')
loo_h_10_10 <- compute_psis_loo(model_hierarchical_10_10)
```

$\sigma_{att} \sim Gamma(5,10)$, $\sigma_{def} \sim Gamma(5,10)$

```{r}
model_hierarchical_5_10 <- readRDS('model_hierarchical_5_10.rds')
loo_h_5_10 <- compute_psis_loo(model_hierarchical_5_10)

loo_compare(loo_h_10_10, loo_h_5_10, loo_hierarchical)
```

# 12. Discussion of issues and potential improvements.


From a footballing perspective, our model is not tacking into account some important aspects that might have an impact the performances on the pitch. In particular, the fatigue coming from the match repetition, especially for the team that are still engaged in other competitions, is not modelled here. For instance, Paris reached the final of the UEFA Champion's League this year while Lyon lost in the semi-finals round against Bayern and both were qualified for the final rounds of the Coupe De France. Without a doubt, this would have had a negative impact on their performances in Ligue 1. 
An other interesting case would be Lille. They struggled a lot during the first half of the season, but when they started playing only once a week after 2 early eliminations from the Champion's League and Coupe De France, they have started winning consistantly and looked like they would be able to continue on the same run of form for the rest of the season. Conversely, Bordeaux did have a solid first half of the season but did not have the same momentum towards the last few games. We believe that managing (somehow) to introduce this idea of momentum inside the model would have give more accurate predictions.


13. Conclusion what was learned from the data analysis.
14. Self-reflection of what the group learned while making the project.

